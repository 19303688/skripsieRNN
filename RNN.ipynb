{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-13.3.1-arm64-arm-64bit\n",
      "Tensor Flow Version: 2.9.0\n",
      "Keras Version: 2.9.0\n",
      "\n",
      "Python 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:26:08) [Clang 14.0.6 ]\n",
      "Pandas 1.5.3\n",
      "Scikit-Learn 1.2.2\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import platform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32768\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "df = pd.read_csv('data_cleaned.csv')\n",
    "df = df.iloc[:, :16]\n",
    "print(len(df.index))\n",
    "\n",
    "#Instead of dropping rows where plant is off, convert the row to zero\n",
    "df.loc[df['Coal Input'] < 30] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply MinMaxScaler to the dataframe\n",
    "scaler = MinMaxScaler()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "\n",
    "#convert the numpy scaled array back to a dataframe\n",
    "df_scaled = pd.DataFrame(scaled_df, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets: 32\n",
      "Total number of rows: 22565\n"
     ]
    }
   ],
   "source": [
    "df_scaled['Power Plant On'] = (df_scaled['Coal Input'] > 0).astype(int)\n",
    "\n",
    "dataframes = []\n",
    "indices = []\n",
    "for index, row in df_scaled.iterrows():\n",
    "    if row['Power Plant On'] == 0:\n",
    "        if indices:\n",
    "            dataframes.append(df_scaled.loc[indices].copy())\n",
    "            indices = []\n",
    "    else:\n",
    "        indices.append(index)\n",
    "\n",
    "# Check if there are any remaining indices after the loop ends\n",
    "if indices:\n",
    "    dataframes.append(df_scaled.loc[indices].copy())\n",
    "\n",
    "# Print the number of different datasets and their lengths\n",
    "print(f\"Number of datasets: {len(dataframes)}\")\n",
    "#for i, dataset in enumerate(dataframes):\n",
    "    #print(f\"Dataset {i+1} length: {len(dataset)}\")\n",
    "\n",
    "# Calculate the total number of rows\n",
    "total_rows = sum(len(dataset) for dataset in dataframes)\n",
    "print(f\"Total number of rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timewindow_df(df, timeBackWindow, timeForwardWindow):\n",
    "    # timeBackWindow = 5\n",
    "    # timeForwardWindow = 2\n",
    "\n",
    "    # Define column names for the first part\n",
    "    cols_part1 = ['ECO SYS FW Supply SATN T', 'Economizer SYS FW Supply P', 'Live Steam TOT F',\n",
    "                  'AMB Air T', 'Coal Input', 'BLR Primary Air F', 'BLR Secondary Air Total F',\n",
    "                  'OVR FIR CORR Air NOZ 71-73 F', 'OVR FIR CORR Air NOZ 74-76 F',\n",
    "                  'OVR FIR CORR Air NOZ 81-83 F', 'OVR FIR CORR Air NOZ 84-86 F']\n",
    "    # Define column names for the second part\n",
    "    cols_part2 = ['HP Steam Average Temp', 'Hot R/H Average Temp', 'ATT 1 m_dot', 'ATT 2 m_dot', 'R/H ATT m_dot']\n",
    "    # Shift rows and concatenate for both parts\n",
    "    df_temp = pd.DataFrame()\n",
    "\n",
    "    for col in cols_part1 + cols_part2:\n",
    "        # Shift columns forward for the first part\n",
    "        if col in cols_part1:\n",
    "            df_new = pd.concat([df[col].shift(-i) for i in range(timeBackWindow-1, -1, -1)], axis=1, \n",
    "                               keys=[f'{col}-{i}' for i in range(timeBackWindow)])\n",
    "        # Shift columns backward for the second part\n",
    "        else:\n",
    "            shifted_cols = [df[col].shift(i) for i in range(timeForwardWindow)]\n",
    "            df_new = pd.concat(shifted_cols, axis=1, \n",
    "                               keys=[f'{col}+{i}' for i in range(timeForwardWindow)])\n",
    "        # Concatenate the resulting dataframe with df_temp, along the columns axis (axis=1)\n",
    "        df_temp = pd.concat([df_temp, df_new], axis=1)\n",
    "\n",
    "    return df_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of manipulated datasets: 32\n",
      "Total number of rows: 22565\n"
     ]
    }
   ],
   "source": [
    "#Time window the datasets\n",
    "manipulated_dataframes = []\n",
    "\n",
    "for df in dataframes:\n",
    "    manipulated_df = timewindow_df(df,5,2)\n",
    "    manipulated_dataframes.append(manipulated_df)\n",
    "\n",
    "\n",
    "# Print the number of rows for each dataset in the manipulated dataframes\n",
    "print(f\"Number of manipulated datasets: {len(manipulated_dataframes)}\")\n",
    "#for i, dataset in enumerate(manipulated_dataframes):\n",
    "    #print(f\"Manipulated Dataset {i+1} length: {len(dataset)}\")\n",
    "\n",
    "# Calculate the total number of rows across all the manipulated datasets\n",
    "total_rows = sum(len(dataset) for dataset in manipulated_dataframes)\n",
    "\n",
    "# Print the total number of rows\n",
    "print(f\"Total number of rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of manipulated datasets: 32\n",
      "Total number of rows: 22412\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with NaN values from each dataframe\n",
    "manipulated_dataframes_cleaned = []\n",
    "for dataset in manipulated_dataframes:\n",
    "    cleaned_dataset = dataset.dropna()\n",
    "    manipulated_dataframes_cleaned.append(cleaned_dataset)\n",
    "\n",
    "# Print the number of rows for each dataset in the cleaned manipulated dataframes\n",
    "print(f\"Number of manipulated datasets: {len(manipulated_dataframes_cleaned)}\")\n",
    "#for i, dataset in enumerate(manipulated_dataframes_cleaned):\n",
    "    #print(f\"Manipulated Dataset {i+1} length: {len(dataset)}\")\n",
    "\n",
    "# Calculate the total number of rows across all the cleaned manipulated datasets\n",
    "total_rows = sum(len(dataset) for dataset in manipulated_dataframes_cleaned)\n",
    "\n",
    "# Print the total number of rows\n",
    "print(f\"Total number of rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat datasets\n",
    "processed_df = pd.concat(manipulated_dataframes_cleaned, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20170, 10)\n"
     ]
    }
   ],
   "source": [
    "#Split data into the test\n",
    "# x_data = df[['ECO SYS FW Supply SATN T', 'Economizer SYS FW Supply P', 'Live Steam TOT F','AMB Air T','Coal Input',\n",
    "#              'BLR Primary Air F','BLR Secondary Air Total F','OVR FIR CORR Air NOZ 71-73 F','OVR FIR CORR Air NOZ 74-76 F',\n",
    "#              'OVR FIR CORR Air NOZ 81-83 F','OVR FIR CORR Air NOZ 84-86 F',]].values\n",
    "# y_data = df[['HP Steam Average Temp', 'Hot R/H Average Temp', 'ATT 1 m_dot', 'ATT 2 m_dot', 'R/H ATT m_dot']].values\n",
    "\n",
    "\n",
    "x_cols = 5 * 11  # Adjust this based on the time windowing performed\n",
    "x_data = processed_df.iloc[:, :x_cols].values\n",
    "y_data = processed_df.iloc[:, x_cols:].values\n",
    "\n",
    "# print(\"True\")\n",
    "# print(x_data.shape)\n",
    "# print(y_data.shape)\n",
    "\n",
    "#print(\"Adapated\")\n",
    "x_train = x_data[:int(0.9*x_data.shape[0]),:]\n",
    "#print(x_train.shape)\n",
    "x_test = x_data[int(0.9*x_data.shape[0]):,:]\n",
    "#print(x_test.shape)\n",
    "y_train = y_data[:int(0.9*y_data.shape[0]),:]\n",
    "print(y_train.shape)\n",
    "y_test = y_data[int(0.9*y_data.shape[0]):,:]\n",
    "##print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderRNN(tf.keras.Model):\n",
    "    def __init__(self, input_dim, latent_dim, output_dim):\n",
    "        super(EncoderDecoderRNN, self).__init__()\n",
    "        self.encoder_gru = GRU(latent_dim, return_state=True)\n",
    "        self.decoder_gru = GRU(latent_dim, return_sequences=True)\n",
    "        self.decoder_dense = Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_inputs = inputs\n",
    "\n",
    "        # Encoder\n",
    "        _, state_h = self.encoder_gru(encoder_inputs)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_outputs = self.decoder_gru(encoder_inputs, initial_state=state_h)\n",
    "        decoder_outputs = self.decoder_dense(decoder_outputs)\n",
    "\n",
    "        return decoder_outputs\n",
    "\n",
    "# Example usage\n",
    "input_dim = 11\n",
    "latent_dim = 32\n",
    "output_dim = 5\n",
    "\n",
    "# Reshape the input data\n",
    "x_train_encoder = x_train.reshape(-1, 5, 11)\n",
    "x_test = x_test.reshape(-1, 5, 11)\n",
    "\n",
    "\n",
    "# Reshape the target data\n",
    "y_train = y_train.reshape(-1, 2, 5)\n",
    "y_test = y_test.reshape(-1, 2, 5)\n",
    "\n",
    "\n",
    "encoder_inputs = tf.keras.Input(shape=(5, input_dim))\n",
    "model = EncoderDecoderRNN(input_dim, latent_dim, output_dim)\n",
    "decoder_outputs = model(encoder_inputs)\n",
    "\n",
    "# Training\n",
    "model.compile(optimizer=RMSprop(), loss='mse')  # Customize optimizer and loss function as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 20170\n  y sizes: 0\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Fit the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(x_test, y_test))\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/data_adapter.py:1655\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1651\u001b[0m   msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1652\u001b[0m       label, \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m   1653\u001b[0m                        \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)))\n\u001b[1;32m   1654\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1655\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 20170\n  y sizes: 0\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
